{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Spark SQL Quiz\n",
    "\n",
    "This quiz uses the same dataset and most of the same questions from the earlier \"Quiz - Data Wrangling with Data Frames Jupyter Notebook.\" For this quiz, however, use Spark SQL instead of Spark Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# TODOS: \n",
    "# 1) import any other libraries you might need\n",
    "# 2) instantiate a Spark session \n",
    "# 3) read in the data set located at the path \"data/sparkify_log_small.json\"\n",
    "# 4) create a view to use with your SQL queries\n",
    "# 5) write code to answer the quiz questions \n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data wrangling with Spark SQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = \"data/sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)\n",
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which page did user id \"\"(empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Pages:\n",
      "{Row(page='Settings'), Row(page='Submit Upgrade'), Row(page='Error'), Row(page='Upgrade'), Row(page='Logout'), Row(page='Submit Downgrade'), Row(page='NextSong'), Row(page='Save Settings'), Row(page='Downgrade')}\n"
     ]
    }
   ],
   "source": [
    "# TODO: write your code to answer question 1\n",
    "\n",
    "pages_visited = spark.sql('''SELECT page FROM user_log_table \n",
    "                            WHERE userId == \"\"''').dropDuplicates()\n",
    "\n",
    "pages_all = spark.sql('''SELECT page FROM user_log_table''').dropDuplicates()\n",
    "\n",
    "pages_unseen = set(pages_all.collect()) - set(pages_visited.collect())\n",
    "\n",
    "print(\"Unseen Pages:\")\n",
    "print(pages_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Reflect\n",
    "\n",
    "Why might you prefer to use SQL over data frames? Why might you prefer data frames over SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL:\n",
    "# - Familiarity for certain users\n",
    "# - Perhaps code can be easier translated to Spark in Java or other scenarios\n",
    "\n",
    "\n",
    "# DFs:\n",
    "# - Familiarity for certain users\n",
    "# - Perhaps code can be easier translated to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|gender|count(userId)|\n",
      "+------+-------------+\n",
      "|     F|          462|\n",
      "|  null|            1|\n",
      "|     M|          501|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: write your code to answer question 3\n",
    "\n",
    "\n",
    "user_log_distinct = spark.sql('''\n",
    "                            SELECT userId, gender \n",
    "                            FROM user_log_table\n",
    "                            ''').dropDuplicates()\n",
    "\n",
    "\n",
    "user_log_distinct.createOrReplaceTempView(\"user_log_distinct\")\n",
    "\n",
    "user_gender_dist = spark.sql('''\n",
    "                            SELECT gender, COUNT(userId)\n",
    "                            FROM user_log_distinct\n",
    "                            GROUP BY gender\n",
    "                            ''')\n",
    "\n",
    "user_gender_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              artist|playsCount|\n",
      "+--------------------+----------+\n",
      "|            Coldplay|        83|\n",
      "|       Kings Of Leon|        69|\n",
      "|Florence + The Ma...|        52|\n",
      "|            BjÃÂ¶rk|        46|\n",
      "|       Dwight Yoakam|        45|\n",
      "|       Justin Bieber|        43|\n",
      "|      The Black Keys|        40|\n",
      "|         OneRepublic|        37|\n",
      "|                Muse|        36|\n",
      "|        Jack Johnson|        36|\n",
      "|           Radiohead|        31|\n",
      "|        Taylor Swift|        29|\n",
      "|Barry Tuckwell/Ac...|        28|\n",
      "|          Lily Allen|        28|\n",
      "|               Train|        28|\n",
      "|           Daft Punk|        27|\n",
      "|           Metallica|        27|\n",
      "|          Nickelback|        27|\n",
      "|          Kanye West|        26|\n",
      "|Red Hot Chili Pep...|        24|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: write your code to answer question 4\n",
    "\n",
    "artist_popularity = spark.sql('''\n",
    "                            SELECT artist, count(userId) AS playsCount\n",
    "                            FROM user_log_table\n",
    "                            WHERE page == \"NextSong\"\n",
    "                            GROUP BY artist\n",
    "                            ORDER BY playsCount DESC\n",
    "                            ''')\n",
    "\n",
    "artist_popularity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count_results)|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;\n",
    "is_home = spark.sql(\"SELECT userID, page, ts, CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home FROM user_log_table \\\n",
    "            WHERE (page = 'NextSong') or (page = 'Home') \\\n",
    "            \")\n",
    "\n",
    "# keep the results in a new view\n",
    "is_home.createOrReplaceTempView(\"is_home_table\")\n",
    "\n",
    "# find the cumulative sum over the is_home column\n",
    "cumulative_sum = spark.sql(\"SELECT *, SUM(is_home) OVER \\\n",
    "    (PARTITION BY userID ORDER BY ts DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS period \\\n",
    "    FROM is_home_table\")\n",
    "\n",
    "# keep the results in a view\n",
    "cumulative_sum.createOrReplaceTempView(\"period_table\")\n",
    "\n",
    "# find the average count for NextSong\n",
    "spark.sql(\"SELECT AVG(count_results) FROM \\\n",
    "          (SELECT COUNT(*) AS count_results FROM period_table \\\n",
    "GROUP BY userID, period, page HAVING page = 'NextSong') AS counts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
